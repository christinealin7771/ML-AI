{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "\n",
    "print(np.where(pd.isnull(train)))\n",
    "print(np.where(pd.isnull(test)))\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Part 2 - Text Preprocessing\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# 1: lower-casing\n",
    "train['Text'] = train['Text'].str.lower()\n",
    "print(train.head())\n",
    "\n",
    "test['Text'] = test['Text'].str.lower()\n",
    "print(test.head())\n",
    "\n",
    "# 2: remove digital numbers\n",
    "\n",
    "import re #python regular expression library\n",
    "\n",
    "train['Text'] = train['Text'].apply(lambda x: re.sub('[0-9]', '', x).strip())\n",
    "test['Text'] = test['Text'].apply(lambda x: re.sub('[0-9]', '', x).strip())\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "\n",
    "\n",
    "# 3: Remove urls\n",
    "                    \n",
    "train['Text'] = train['Text'].apply(lambda x: re.sub('http\\S+', ' ', x).strip())\n",
    "test['Text'] = test['Text'].apply(lambda x: re.sub('http\\S+', ' ', x).strip())\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "\n",
    "\n",
    "# 4: Remove username\n",
    "\n",
    "train['Text'] = train['Text'].apply(lambda x: re.sub('@[^\\s]+', '', x).strip())\n",
    "test['Text'] = test['Text'].apply(lambda x: re.sub('@[^\\s]+', '', x).strip())\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "\n",
    "\n",
    "\n",
    "# 5: Remove special character and puncation \n",
    "train['Text'] = train['Text'].apply(lambda x: re.sub('[^a-z0-9<>\\']', ' ', x).strip())\n",
    "test['Text'] = test['Text'].apply(lambda x: re.sub('[^a-z0-9<>\\']', ' ', x).strip())\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer=SnowballStemmer(\"english\") #define stemming dict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def stem_sentences(sentence):\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "train['Text'] = train['Text'].apply(lambda x : stem_sentences(x))\n",
    "test['Text'] = test['Text'].apply(lambda x : stem_sentences(x))\n",
    "\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "\n",
    "\n",
    "# Get sample size\n",
    "\n",
    "train = train.sample(frac=0.01, random_state=1)\n",
    "test = test.sample(frac=0.01, random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Part 3 - Linguistic Feature Extraction\n",
    "\n",
    "'''\n",
    "\n",
    "# 1. Bag of Words (train.csv)\n",
    "import numpy as np\n",
    "\n",
    "wordCount = {}\n",
    "for tokens in train['Text']:\n",
    "    for word in tokens:\n",
    "        if word not in wordCount:\n",
    "            wordCount[word] = 1\n",
    "        else:\n",
    "            wordCount[word] += 1\n",
    "\n",
    "unique_words = list(wordCount.keys())\n",
    "\n",
    "print(unique_words)\n",
    "print(len(unique_words))\n",
    "\n",
    "bag_of_words = []\n",
    "\n",
    "for tokens in train['Text']:\n",
    "    bag_vector = np.zeros(len(unique_words))\n",
    "    for words in tokens:\n",
    "        for i, word in enumerate(unique_words):\n",
    "            if word == words:\n",
    "                bag_vector[i] += 1\n",
    "    bag_of_words.append(bag_vector.tolist())\n",
    "\n",
    "\n",
    "print(bag_of_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Bag of Words (test.csv)\n",
    "import numpy as np\n",
    "\n",
    "wordCount = {}\n",
    "for tokens in test['Text']:\n",
    "    for word in tokens:\n",
    "        if word not in wordCount:\n",
    "            wordCount[word] = 1\n",
    "        else:\n",
    "            wordCount[word] += 1\n",
    "\n",
    "unique_words2 = list(wordCount.keys())\n",
    "\n",
    "print(unique_words2)\n",
    "print(len(unique_words2))\n",
    "\n",
    "bag_of_words_test = []\n",
    "\n",
    "for tokens in test['Text']:\n",
    "    bag_vector = np.zeros(len(unique_words2))\n",
    "    for words in tokens:\n",
    "        for i, word in enumerate(unique_words2):\n",
    "            if word == words:\n",
    "                bag_vector[i] += 1\n",
    "    bag_of_words_test.append(bag_vector.tolist())\n",
    "\n",
    "\n",
    "print(bag_of_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. TF*IDF (train.csv)\n",
    "\n",
    "# Find term-frequency: num of that word in sentence/ number of words in sentence\n",
    "\n",
    "num_comments = len(train['Text'])\n",
    "train_list = train['Text'].tolist()\n",
    "\n",
    "print(train_list)\n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((num_comments, len(unique_words))), columns = unique_words)\n",
    "\n",
    "for i in range(num_comments):\n",
    "    for w in train_list[i]:\n",
    "        df_tf[w][i] = df_tf[w][i] + (1/len(train_list[i]))\n",
    "\n",
    "print(\"Term Frequency: \\n\", df_tf)\n",
    "\n",
    "\n",
    "# Inverse Document Frequency: log(num of comments / word in all sentences)\n",
    "idf = {}\n",
    "\n",
    "for w in unique_words:\n",
    "    k = 0 \n",
    "\n",
    "    for i in range(num_comments):\n",
    "        if w in train_list[i]:\n",
    "            k += 1\n",
    "    \n",
    "    idf[w] = np.log10(num_comments/k)\n",
    "\n",
    "print(\"IDF of: \\n\", idf)\n",
    "\n",
    "# TF*DF = term-frequency * Inverse Document Frequency \n",
    "\n",
    "df_tf_idf = df_tf.copy()\n",
    "for w in unique_words:\n",
    "    for i in range(num_comments):\n",
    "        df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "\n",
    "\n",
    "print(\"TF*DF of: \\n\", df_tf_idf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. TF*IDF (test.csv)\n",
    "\n",
    "# Find term-frequency: num of that word/ number of words in sentence\n",
    "\n",
    "\n",
    "num_comments = len(test['Text'])\n",
    "test_list = test['Text'].tolist()\n",
    "\n",
    "print(test_list)\n",
    "\n",
    "df_tf2 = pd.DataFrame(np.zeros((num_comments, len(unique_words2))), columns = unique_words2)\n",
    "\n",
    "for i in range(num_comments):\n",
    "    for w in test_list[i]:\n",
    "        df_tf2[w][i] = df_tf2[w][i] + (1/len(test_list[i]))\n",
    "\n",
    "print(\"Term Frequency: \\n\", df_tf2)\n",
    "\n",
    "\n",
    "# Inverse Document Frequency: log(num of comments / word in all sentences)\n",
    "idf2 = {}\n",
    "\n",
    "for w in unique_words2:\n",
    "    k = 0 \n",
    "\n",
    "    for i in range(num_comments):\n",
    "        if w in test_list[i]:\n",
    "            k += 1\n",
    "    \n",
    "    idf2[w] = np.log10(num_comments/k)\n",
    "\n",
    "print(\"IDF of: \\n\", idf2)\n",
    "\n",
    "# TF*DF = term-frequency * Inverse Document Frequency \n",
    "\n",
    "df_tf_idf2 = df_tf2.copy()\n",
    "for w in unique_words2:\n",
    "    for i in range(num_comments):\n",
    "        df_tf_idf2[w][i] = df_tf2[w][i] * idf2[w]\n",
    "\n",
    "\n",
    "print(\"TF*DF of: \\n\", df_tf_idf2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Word2Vec (train.csv)\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "train_list = train['Text'].tolist()\n",
    "\n",
    "model = Word2Vec(train_list, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Word2Vec (test.csv)\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "test_list = test['Text'].tolist()\n",
    "\n",
    "model = Word2Vec(test_list, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Part 4 - Sentiment Classification Model\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, roc_curve, auc, classification_report\n",
    "\n",
    "\n",
    "\n",
    "#train.csv for bag_of_words\n",
    "x_bag_train = bag_of_words\n",
    "print(bag_of_words)\n",
    "y_bag_train = np.array(train['Sentiment'])\n",
    "print(np.array(train['Sentiment']))\n",
    "\n",
    "# #test.csv for bag_of_words\n",
    "# x_bag_test = bag_of_words_test\n",
    "# y_bag_test = test['Sentiment']\n",
    "\n",
    "\n",
    "lc = LogisticRegression()\n",
    "# svc = SVC(probability=True)\n",
    "# nbc = GaussianNB()\n",
    "# rfc = RandomForestClassifier()\n",
    "\n",
    "# lc.fit(x_bag_train, y_bag_train)\n",
    "# svc.fit(x_bag_train, y_bag_train)\n",
    "# nbc.fit(x_bag_train, y_bag_train)\n",
    "# rfc.fit(x_bag_train, y_bag_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_lc_predicted = lc.predict(x_bag_test)\n",
    "print(classification_report(y_bag_test, y_lc_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
