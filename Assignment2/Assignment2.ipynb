{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "\n",
    "print(np.where(pd.isnull(train)))\n",
    "print(np.where(pd.isnull(test)))\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Part 2 - Text Preprocessing\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# 1: lower-casing\n",
    "train['Text'] = train['Text'].str.lower()\n",
    "print(train.head())\n",
    "\n",
    "test['Text'] = test['Text'].str.lower()\n",
    "print(test.head())\n",
    "\n",
    "# 2: remove digital numbers\n",
    "\n",
    "import re #python regular expression library\n",
    "\n",
    "train['Text'] = train['Text'].apply(lambda x: re.sub('[0-9]', '', x).strip())\n",
    "test['Text'] = test['Text'].apply(lambda x: re.sub('[0-9]', '', x).strip())\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "\n",
    "\n",
    "# 3: Remove urls\n",
    "                    \n",
    "train['Text'] = train['Text'].apply(lambda x: re.sub('http\\S+', ' ', x).strip())\n",
    "test['Text'] = test['Text'].apply(lambda x: re.sub('http\\S+', ' ', x).strip())\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "\n",
    "\n",
    "# 4: Remove username\n",
    "\n",
    "train['Text'] = train['Text'].apply(lambda x: re.sub('@[^\\s]+', '', x).strip())\n",
    "test['Text'] = test['Text'].apply(lambda x: re.sub('@[^\\s]+', '', x).strip())\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "\n",
    "\n",
    "\n",
    "# 5: Remove special character and puncation \n",
    "train['Text'] = train['Text'].apply(lambda x: re.sub('[^a-z0-9<>\\']', ' ', x).strip())\n",
    "test['Text'] = test['Text'].apply(lambda x: re.sub('[^a-z0-9<>\\']', ' ', x).strip())\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: Stemming\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer=SnowballStemmer(\"english\") #define stemming dict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def stem_sentences(sentence):\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "train['Text'] = train['Text'].apply(lambda x : stem_sentences(x))\n",
    "test['Text'] = test['Text'].apply(lambda x : stem_sentences(x))\n",
    "\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "\n",
    "\n",
    "# Get sample size\n",
    "\n",
    "train = train.sample(frac=0.001, random_state=1)\n",
    "# test = test.sample(frac=0.005, random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Part 3 - Linguistic Feature Extraction\n",
    "\n",
    "'''\n",
    "\n",
    "# 1. Bag of Words (train.csv)\n",
    "import numpy as np\n",
    "\n",
    "wordCount = {}\n",
    "for tokens in train['Text']:\n",
    "    for word in tokens:\n",
    "        if word not in wordCount:\n",
    "            wordCount[word] = 1\n",
    "        else:\n",
    "            wordCount[word] += 1\n",
    "\n",
    "unique_words = list(wordCount.keys())\n",
    "\n",
    "bag_of_words = []\n",
    "\n",
    "for tokens in train['Text']:\n",
    "    bag_vector = np.zeros(len(unique_words))\n",
    "    for words in tokens:\n",
    "        for i, word in enumerate(unique_words):\n",
    "            if word == words:\n",
    "                bag_vector[i] += 1\n",
    "    bag_of_words.append(bag_vector.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Bag of Words (test.csv)\n",
    "import numpy as np\n",
    "\n",
    "wordCount = {}\n",
    "for tokens in test['Text']:\n",
    "    for word in tokens:\n",
    "        if word not in wordCount:\n",
    "            wordCount[word] = 1\n",
    "        else:\n",
    "            wordCount[word] += 1\n",
    "\n",
    "unique_words2 = list(wordCount.keys())\n",
    "\n",
    "print(unique_words2)\n",
    "print(len(unique_words2))\n",
    "\n",
    "bag_of_words_test = []\n",
    "\n",
    "for tokens in test['Text']:\n",
    "    bag_vector = np.zeros(len(unique_words2))\n",
    "    for words in tokens:\n",
    "        for i, word in enumerate(unique_words2):\n",
    "            if word == words:\n",
    "                bag_vector[i] += 1\n",
    "    bag_of_words_test.append(bag_vector.tolist())\n",
    "\n",
    "\n",
    "print(bag_of_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. TF*IDF (train.csv)\n",
    "\n",
    "# Find term-frequency: num of that word in sentence/ number of words in sentence\n",
    "\n",
    "num_comments = len(train['Text'])\n",
    "train_list = train['Text'].tolist()\n",
    "\n",
    "# print(train_list)\n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((num_comments, len(unique_words))), columns = unique_words)\n",
    "\n",
    "for i in range(num_comments):\n",
    "    for w in train_list[i]:\n",
    "        df_tf[w][i] = df_tf[w][i] + (1/len(train_list[i]))\n",
    "\n",
    "# print(\"Term Frequency: \\n\", df_tf)\n",
    "\n",
    "\n",
    "# Inverse Document Frequency: log(num of comments / word in all sentences)\n",
    "idf = {}\n",
    "\n",
    "for w in unique_words:\n",
    "    k = 0 \n",
    "\n",
    "    for i in range(num_comments):\n",
    "        if w in train_list[i]:\n",
    "            k += 1\n",
    "    \n",
    "    idf[w] = np.log10(num_comments/k)\n",
    "\n",
    "# print(\"IDF of: \\n\", idf)\n",
    "\n",
    "# TF*DF = term-frequency * Inverse Document Frequency \n",
    "\n",
    "df_tf_idf = df_tf.copy()\n",
    "for w in unique_words:\n",
    "    for i in range(num_comments):\n",
    "        df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "\n",
    "\n",
    "# print(\"TF*DF of: \\n\", df_tf_idf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. TF*IDF (test.csv)\n",
    "\n",
    "# Find term-frequency: num of that word/ number of words in sentence\n",
    "\n",
    "\n",
    "num_comments = len(test['Text'])\n",
    "test_list = test['Text'].tolist()\n",
    "\n",
    "# print(test_list)\n",
    "\n",
    "df_tf2 = pd.DataFrame(np.zeros((num_comments, len(unique_words2))), columns = unique_words2)\n",
    "\n",
    "for i in range(num_comments):\n",
    "    for w in test_list[i]:\n",
    "        df_tf2[w][i] = df_tf2[w][i] + (1/len(test_list[i]))\n",
    "\n",
    "# print(\"Term Frequency: \\n\", df_tf2)\n",
    "\n",
    "\n",
    "# Inverse Document Frequency: log(num of comments / word in all sentences)\n",
    "idf2 = {}\n",
    "\n",
    "for w in unique_words2:\n",
    "    k = 0 \n",
    "\n",
    "    for i in range(num_comments):\n",
    "        if w in test_list[i]:\n",
    "            k += 1\n",
    "    \n",
    "    idf2[w] = np.log10(num_comments/k)\n",
    "\n",
    "# print(\"IDF of: \\n\", idf2)\n",
    "\n",
    "# TF*DF = term-frequency * Inverse Document Frequency \n",
    "\n",
    "df_tf_idf2 = df_tf2.copy()\n",
    "for w in unique_words2:\n",
    "    for i in range(num_comments):\n",
    "        df_tf_idf2[w][i] = df_tf2[w][i] * idf2[w]\n",
    "\n",
    "\n",
    "# print(\"TF*DF of: \\n\", df_tf_idf2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Word2Vec (train.csv)\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "train_list = train['Text'].tolist()\n",
    "\n",
    "model = Word2Vec(train_list, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "word_model_train = []\n",
    "vocab = model.wv.key_to_index.keys()\n",
    "# word_model_train.append(vocab)\n",
    "\n",
    "for sentence in train_list:\n",
    "    temp = []\n",
    "    for val in vocab:\n",
    "        if val in sentence:\n",
    "            temp.append(sum(model.wv[val]))\n",
    "        else:\n",
    "            temp.append(0)\n",
    "    word_model_train.append(temp)\n",
    "\n",
    "print(len(word_model_train[0]))\n",
    "print(len(train_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Word2Vec (test.csv)\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "test_list = test['Text'].tolist()\n",
    "\n",
    "model = Word2Vec(test_list, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vectest.model\")\n",
    "\n",
    "print(model)\n",
    "\n",
    "word_model_test = []\n",
    "vocab = model.wv.key_to_index.keys()\n",
    "# word_model_test.append(vocab)\n",
    "\n",
    "for sentence in test_list:\n",
    "    temp = []\n",
    "    for val in vocab:\n",
    "        if val in sentence:\n",
    "            temp.append(sum(model.wv[val]))\n",
    "        else:\n",
    "            temp.append(0)\n",
    "    word_model_test.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined Bag_of_words into dataframe for model training\n",
    "df = pd.DataFrame(bag_of_words, columns = unique_words)\n",
    "df2 = pd.DataFrame(bag_of_words_test, columns = unique_words2)\n",
    "\n",
    "df_merged_bag = pd.concat([df, df2], axis=0).reset_index(drop=True)\n",
    "df_merged_bag.fillna(0, inplace=True)\n",
    "df_merged2_bag = pd.concat([test, train], axis=0).reset_index(drop=True)\n",
    "\n",
    "# print(df_merged_bag.shape)\n",
    "# print(df_merged2_bag.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined TF*IDF into dataframe for model training\n",
    "df = pd.DataFrame(df_tf_idf, columns = unique_words)\n",
    "df2 = pd.DataFrame(df_tf_idf2, columns = unique_words2)\n",
    "\n",
    "df_merged_tf_idf = pd.concat([df, df2], axis=0).reset_index(drop=True)\n",
    "df_merged_tf_idf.fillna(0, inplace=True)\n",
    "df_merged2_tf_idf = pd.concat([test, train], axis=0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined word2Vec into dataframe for model training\n",
    "df = pd.DataFrame(word_model_train, columns = unique_words)\n",
    "df2 = pd.DataFrame(word_model_test, columns = unique_words2)\n",
    "\n",
    "print(df.shape)\n",
    "print(df2.shape)\n",
    "\n",
    "df_merged_word2vec = pd.concat([df, df2], axis=0).reset_index(drop=True)\n",
    "df_merged_word2vec.fillna(0, inplace=True)\n",
    "df_merged2_word2vec = pd.concat([test, train], axis=0).reset_index(drop=True)\n",
    "\n",
    "print(df_merged_word2vec.shape)\n",
    "print(df_merged2_word2vec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Part 4 - Sentiment Classification Model\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, roc_curve, auc, classification_report\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1: Classification Model for Bag_of_Words\n",
    "\n",
    "y = df_merged2_bag['Sentiment'].to_numpy()\n",
    "X = df_merged_bag\n",
    "\n",
    "\n",
    "# Data Scaling\n",
    "scale = StandardScaler()\n",
    "scaled_X = scale.fit_transform(X)\n",
    "\n",
    "x_bag_train, x_bag_test,  y_bag_train, y_bag_test = train_test_split(scaled_X, y, test_size = 0.3)\n",
    "\n",
    "lc = LogisticRegression()\n",
    "svc = SVC()\n",
    "nbc = GaussianNB()\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "\n",
    "lc.fit(x_bag_train, y_bag_train)\n",
    "svc.fit(x_bag_train, y_bag_train)\n",
    "nbc.fit(x_bag_train, y_bag_train)\n",
    "rfc.fit(x_bag_train, y_bag_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_lc_predicted = lc.predict(x_bag_test)\n",
    "print(classification_report(y_bag_test, y_lc_predicted))\n",
    "print(lc.score(x_bag_test,y_bag_test))\n",
    "\n",
    "y_svc_predicted = svc.predict(x_bag_test)\n",
    "print(classification_report(y_bag_test, y_svc_predicted))\n",
    "print(svc.score(x_bag_test,y_bag_test))\n",
    "\n",
    "y_nbc_predicted = nbc.predict(x_bag_test)\n",
    "print(classification_report(y_bag_test, y_nbc_predicted))\n",
    "print(nbc.score(x_bag_test,y_bag_test))\n",
    "\n",
    "y_rfc_predicted = rfc.predict(x_bag_test)\n",
    "print(classification_report(y_bag_test, y_rfc_predicted))\n",
    "print(rfc.score(x_bag_test,y_bag_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Classification Model for TF*IDF\n",
    "\n",
    "y = df_merged2_tf_idf['Sentiment'].to_numpy()\n",
    "X = df_merged_tf_idf\n",
    "\n",
    "\n",
    "# Data Scaling\n",
    "# scale = preprocessing.MinMaxScaler()\n",
    "scale = StandardScaler()\n",
    "scaled_X = scale.fit_transform(X)\n",
    "\n",
    "x_tf_idf_train, x_tf_idf_test,  y_tf_idf_train, y_tf_idf_test = train_test_split(scaled_X, y, test_size = 0.3)\n",
    "\n",
    "lc = LogisticRegression()\n",
    "svc = SVC()\n",
    "nbc = GaussianNB()\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "\n",
    "lc.fit(x_tf_idf_train, y_tf_idf_train)\n",
    "svc.fit(x_tf_idf_train, y_tf_idf_train)\n",
    "nbc.fit(x_tf_idf_train, y_tf_idf_train)\n",
    "rfc.fit(x_tf_idf_train, y_tf_idf_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lc_predicted = lc.predict(x_tf_idf_test)\n",
    "print(classification_report(y_tf_idf_test, y_lc_predicted))\n",
    "print(lc.score(x_tf_idf_test,y_tf_idf_test))\n",
    "\n",
    "y_svc_predicted = svc.predict(x_tf_idf_test)\n",
    "print(classification_report(y_tf_idf_test, y_svc_predicted))\n",
    "print(svc.score(x_tf_idf_test,y_tf_idf_test))\n",
    "\n",
    "y_nbc_predicted = nbc.predict(x_tf_idf_test)\n",
    "print(classification_report(y_tf_idf_test, y_nbc_predicted))\n",
    "print(nbc.score(x_tf_idf_test,y_tf_idf_test))\n",
    "\n",
    "y_rfc_predicted = rfc.predict(x_tf_idf_test)\n",
    "print(classification_report(y_tf_idf_test, y_rfc_predicted))\n",
    "print(rfc.score(x_tf_idf_test,y_tf_idf_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Classification Model for Word2Vec\n",
    "\n",
    "\n",
    "y = df_merged2_word2vec['Sentiment'].to_numpy()\n",
    "X = df_merged_word2vec\n",
    "\n",
    "\n",
    "# Data Scaling\n",
    "# scale = preprocessing.MinMaxScaler()\n",
    "scale = StandardScaler()\n",
    "scaled_X = scale.fit_transform(X)\n",
    "\n",
    "x_word2vec_train, x_word2vec_test,  y_word2vec_train, y_word2vec_test = train_test_split(scaled_X, y, test_size = 0.3)\n",
    "\n",
    "lc = LogisticRegression()\n",
    "svc = SVC()\n",
    "nbc = GaussianNB()\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "\n",
    "lc.fit(x_word2vec_train, y_word2vec_train)\n",
    "svc.fit(x_word2vec_train, y_word2vec_train)\n",
    "nbc.fit(x_word2vec_train, y_word2vec_train)\n",
    "rfc.fit(x_word2vec_train, y_word2vec_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lc_predicted = lc.predict(x_word2vec_test)\n",
    "print(classification_report(y_word2vec_test, y_lc_predicted))\n",
    "print(lc.score(x_word2vec_test,y_word2vec_test))\n",
    "\n",
    "y_svc_predicted = svc.predict(x_word2vec_test)\n",
    "print(classification_report(y_word2vec_test, y_svc_predicted))\n",
    "print(svc.score(x_word2vec_test,y_word2vec_test))\n",
    "\n",
    "y_nbc_predicted = nbc.predict(x_word2vec_test)\n",
    "print(classification_report(y_word2vec_test, y_nbc_predicted))\n",
    "print(nbc.score(x_word2vec_test,y_word2vec_test))\n",
    "\n",
    "y_rfc_predicted = rfc.predict(x_word2vec_test)\n",
    "print(classification_report(y_word2vec_test, y_rfc_predicted))\n",
    "print(rfc.score(x_word2vec_test,y_word2vec_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
